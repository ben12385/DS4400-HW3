{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preped\n"
     ]
    }
   ],
   "source": [
    "#Extract Data\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "\n",
    "datasetFile = open('./spambase.data', 'r')\n",
    "\n",
    "spam = list()\n",
    "notSpam = list()\n",
    "\n",
    "\n",
    "for line in datasetFile:\n",
    "    splitedLine = line.split(',')\n",
    "    for a in range(0, len(splitedLine)):\n",
    "        splitedLine[a] = float(splitedLine[a])\n",
    "    if(splitedLine[-1] == 1):\n",
    "        spam.append(splitedLine)\n",
    "    else:\n",
    "        notSpam.append(splitedLine)\n",
    "\n",
    "#Randomize\n",
    "random.shuffle(spam)\n",
    "random.shuffle(notSpam)\n",
    "\n",
    "training = copy.deepcopy(spam[0:1359])\n",
    "\n",
    "notSpamTraining = copy.deepcopy(notSpam[0:2091])\n",
    "training.extend(notSpamTraining)\n",
    "\n",
    "testing = copy.deepcopy(spam[1359:-1])\n",
    "\n",
    "notSpamTesting = copy.deepcopy(notSpam[2091:-1])\n",
    "testing.extend(notSpamTesting)\n",
    "\n",
    "random.shuffle(training)\n",
    "random.shuffle(testing)\n",
    "\n",
    "   \n",
    "trainingX = np.array([row[:-1] for row in training])\n",
    "trainingY = np.array([row[-1] for row in training])\n",
    "\n",
    "testingX = np.array([row[:-1] for row in testing])\n",
    "testingY = np.array([row[-1] for row in testing])\n",
    "\n",
    "discreteTrainingX = list()\n",
    "for a in trainingX:\n",
    "    b = (a > 0).astype(int)[:-3]\n",
    "    b = np.append(b, a[-3:])\n",
    "    discreteTrainingX.append(b)\n",
    "\n",
    "discreteTestX = list()\n",
    "for a in testingX:\n",
    "    b = (a > 0).astype(int)[:-3]\n",
    "    b = np.append(b, a[-3:])\n",
    "    discreteTestX.append(b)    \n",
    "    \n",
    "print(\"Data Preped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10\n",
      "For training set\n",
      "Accuracy 0.9974\n",
      "Error 0.0026\n",
      "Precision: 0.9971\n",
      "Recall: 0.9963\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9487\n",
      "Error 0.0513\n",
      "Precision: 0.9417\n",
      "Recall: 0.9272\n",
      "\n",
      "Random Forest with 50\n",
      "For training set\n",
      "Accuracy 0.9994\n",
      "Error 0.0006\n",
      "Precision: 1.0000\n",
      "Recall: 0.9985\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9582\n",
      "Error 0.0418\n",
      "Precision: 0.9451\n",
      "Recall: 0.9492\n",
      "\n",
      "Random Forest with 100\n",
      "For training set\n",
      "Accuracy 0.9994\n",
      "Error 0.0006\n",
      "Precision: 1.0000\n",
      "Recall: 0.9985\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9600\n",
      "Error 0.0400\n",
      "Precision: 0.9492\n",
      "Recall: 0.9492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sklearn's Random Forest\n",
    "for numOfTrees in [10, 50 ,100]:\n",
    "    clf = RandomForestClassifier(n_estimators=numOfTrees)\n",
    "    clf = clf.fit(discreteTrainingX, trainingY)\n",
    "\n",
    "    print(\"Random Forest with %d\" % numOfTrees)\n",
    "    #Metrics\n",
    "    y_pred = clf.predict(discreteTrainingX)\n",
    "    confMatrix = metrics.confusion_matrix(trainingY, y_pred)\n",
    "    accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "    precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "    recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    print(\"For training set\")\n",
    "    print(\"Accuracy %.4f\" % accuracy)\n",
    "    print(\"Error %.4f\" % (1-accuracy))\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print()\n",
    "\n",
    "\n",
    "    y_pred = clf.predict(discreteTestX)\n",
    "    confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "    accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "    precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "    recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    print(\"For testing set\")\n",
    "    print(\"Accuracy %.4f\" % accuracy)\n",
    "    print(\"Error %.4f\" % (1-accuracy))\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Own Random Forest Classification Creation\n",
    "def calculateFeatureToUse(mergedIO, featuresSelected):\n",
    "    #Use Gini\n",
    "    giniList = list()\n",
    "    for feature in featuresSelected:     \n",
    "        featureIs0AndOutputIs0 = 0\n",
    "        featureIs0AndOutputIs1 = 0\n",
    "        featureIs1AndOutputIs0 = 0\n",
    "        featureIs1AndOutputIs1 = 0\n",
    "        #If feature value is less than 54 then it is discrete 0 and 1\n",
    "        if(feature < 54):\n",
    "            for example in mergedIO:\n",
    "                if example[feature] == 0 and example[-1] == 0:\n",
    "                    featureIs0AndOutputIs0 += 1\n",
    "                elif example[feature] == 0 and example[-1] == 1:\n",
    "                    featureIs0AndOutputIs1 += 1\n",
    "                elif example[feature] == 1 and example[-1] == 0:\n",
    "                    featureIs1AndOutputIs0 += 1\n",
    "                elif example[feature] == 1 and example[-1] == 1:\n",
    "                    featureIs1AndOutputIs1 += 1\n",
    "             \n",
    "            feature0 = 0\n",
    "            if(featureIs0AndOutputIs0 != 0 and featureIs0AndOutputIs1 != 0):\n",
    "                feature0Output0 = (featureIs0AndOutputIs0/(featureIs0AndOutputIs0+featureIs0AndOutputIs1))\n",
    "                feature0Output1 = (featureIs0AndOutputIs1/(featureIs0AndOutputIs0+featureIs0AndOutputIs1))\n",
    "                feature0 = (1-(feature0Output0**2+feature0Output1**2)) * ((featureIs0AndOutputIs0+featureIs0AndOutputIs1)/len(mergedIO))\n",
    "            \n",
    "            feature1 = 0\n",
    "            if(featureIs1AndOutputIs0 != 0 or featureIs1AndOutputIs1 != 0):\n",
    "                feature1Output0 = (featureIs1AndOutputIs0/(featureIs1AndOutputIs0+featureIs1AndOutputIs1))\n",
    "                feature1Output1 = (featureIs1AndOutputIs1/(featureIs1AndOutputIs0+featureIs1AndOutputIs1))\n",
    "                feature1 = (1-(feature1Output0**2+feature1Output1**2)) * ((featureIs1AndOutputIs0+featureIs1AndOutputIs1)/len(mergedIO))\n",
    "            \n",
    "            giniIndex = feature0 + feature1\n",
    "            \n",
    "            giniList.append(giniIndex)\n",
    "        else:\n",
    "            print(\"This should never happen\")\n",
    "            \n",
    "\n",
    "    location = np.argmin(giniList)\n",
    "    return featuresSelected[location], giniList[location]\n",
    "\n",
    "def generateTree(mergedIO, numFeaturesToUse, tree, featureSelection):\n",
    "    #Structure(featureToUse, howToSplit, leftLeafIndex, feature = 0, RightLeafIndex, feature = 1)\n",
    "    random.shuffle(featureSelection)\n",
    "    featureToUse, giniIndex = calculateFeatureToUse(mergedIO, featureSelection[:numFeaturesToUse])\n",
    "    featureSelection.remove(featureToUse)\n",
    "    \n",
    "    \n",
    "    indexForEdit = len(tree)   \n",
    "    #Perfect Split\n",
    "    if(giniIndex == 0):       \n",
    "        tree.append([featureToUse, 0, False, True])\n",
    "        return indexForEdit\n",
    "            \n",
    "    tree.append([featureToUse, 0, 0, 0])\n",
    "    \n",
    "    #Split the set\n",
    "    lessThanSet = list()\n",
    "    moreThanSet = list()\n",
    "    for example in mergedIO:\n",
    "        if(example[featureToUse] == 0):\n",
    "            lessThanSet.append(example)\n",
    "        elif(example[featureToUse] == 1):\n",
    "            moreThanSet.append(example)\n",
    "        else:\n",
    "            print(\"Split Set Sanity Check\")\n",
    "    lessThanSet = np.array(lessThanSet)\n",
    "    moreThanSet = np.array(moreThanSet) \n",
    "    \n",
    "    if(len(featureSelection) == 0):\n",
    "        #No more features to test so end\n",
    "        (values,counts) = np.unique(lessThanSet[:,-1],return_counts=True)\n",
    "        location=np.argmax(counts)\n",
    "        tree[indexForEdit][2] = bool(values[location])\n",
    "\n",
    "        (values,counts) = np.unique(moreThanSet[:,-1],return_counts=True)\n",
    "        location=np.argmax(counts)\n",
    "        tree[indexForEdit][3] = bool(values[location])\n",
    "        print(\"No more features\")\n",
    "        return indexForEdit\n",
    "    \n",
    "    \n",
    "    #Check if totally one sided\n",
    "    #If when feature is 0, then find most common output for when feature is 1 and opposite for when feature is 0\n",
    "    if(len(lessThanSet) == 0):\n",
    "        (values,counts) = np.unique(moreThanSet[:,-1],return_counts=True)\n",
    "        location=np.argmax(counts)\n",
    "        tree[indexForEdit][2] = not bool(values[location])\n",
    "        tree[indexForEdit][3] = bool(values[location])\n",
    "        return indexForEdit\n",
    "    #If when feature is 1, then find most common output for when feature is 0 and opposite for when feature is 1\n",
    "    elif(len(moreThanSet) == 0):\n",
    "        (values,counts) = np.unique(lessThanSet[:,-1],return_counts=True)\n",
    "        location=np.argmax(counts)\n",
    "        tree[indexForEdit][2] = bool(values[location])\n",
    "        tree[indexForEdit][3] = not bool(values[location])\n",
    "        return indexForEdit\n",
    "            \n",
    "    #Do left node (False)\n",
    "    leftNodeLocation = generateTree(lessThanSet, numFeaturesToUse, tree, copy.deepcopy(featureSelection))\n",
    "    tree[indexForEdit][2] = leftNodeLocation\n",
    "    \n",
    "    rightNodeLocation = generateTree(moreThanSet, numFeaturesToUse, tree, copy.deepcopy(featureSelection))\n",
    "    tree[indexForEdit][3] = rightNodeLocation                  \n",
    "    \n",
    "    return indexForEdit\n",
    "        \n",
    "        \n",
    "\n",
    "def randomForest(x, y, numOfTrees=10, featuresToUse=1):\n",
    "\n",
    "    #Calculate number of features\n",
    "    numOfFeatures = len(x[0])\n",
    "\n",
    "    if featuresToUse > numOfFeatures:\n",
    "        print(\"Error in number of features\")\n",
    "        return\n",
    "    \n",
    "    trees = list()\n",
    "    for a in range(0, numOfTrees):\n",
    "        #Bootstrap sampling\n",
    "        bootStrapX, bootStrapY = utils.resample(x, y, replace=False)\n",
    "        \n",
    "        #Merge input and output\n",
    "        mergedFeatureOutput = np.append(x, np.array([y]).transpose(), axis=1)\n",
    "        \n",
    "        #Create tree and store\n",
    "        treeCreated = list()\n",
    "        featureSelection = [a for a in range(0, len(mergedFeatureOutput[0])-4)]\n",
    "        \n",
    "        #-4 to skip the last 3\n",
    "        generateTree(mergedFeatureOutput, featuresToUse, treeCreated, featureSelection)\n",
    "        trees.append(treeCreated)\n",
    "    \n",
    "    return trees\n",
    "\n",
    "\n",
    "trees = randomForest(discreteTrainingX, trainingY, 10, 7)\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traverse(example, tree):\n",
    "    nextToCheck = 0\n",
    "    \n",
    "    while(True):\n",
    "        data = tree[nextToCheck]\n",
    "        featureToCheck = data[0]\n",
    "        leftNodeLocation = data[2]\n",
    "        rightNodeLocation = data[3]\n",
    "\n",
    "        if example[featureToCheck] == 0:\n",
    "            if(type(leftNodeLocation) is not bool):\n",
    "                nextToCheck = leftNodeLocation\n",
    "            else:\n",
    "                return leftNodeLocation\n",
    "        elif example[featureToCheck] == 1:\n",
    "            if(type(rightNodeLocation) is not bool):\n",
    "                nextToCheck = rightNodeLocation\n",
    "            else:\n",
    "                return rightNodeLocation\n",
    "            \n",
    "def predictRandomForest(testX, trees):\n",
    "    predictY = list()\n",
    "    for example in testX:\n",
    "        vote = list()\n",
    "        for tree in trees:\n",
    "            vote.append(traverse(example, tree))\n",
    "        \n",
    "        #print(vote)\n",
    "        (values,counts) = np.unique(vote,return_counts=True)\n",
    "        location=np.argmax(counts)\n",
    "        predictY.append(int(values[location]))\n",
    "        \n",
    "    return predictY\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10\n",
      "For training set\n",
      "Accuracy 0.6067\n",
      "Error 0.3933\n",
      "Precision: 0.5063\n",
      "Recall: 0.0589\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.6084\n",
      "Error 0.3916\n",
      "Precision: 0.5429\n",
      "Recall: 0.0419\n",
      "\n",
      "Random Forest with 50\n",
      "For training set\n",
      "Accuracy 0.6119\n",
      "Error 0.3881\n",
      "Precision: 0.5877\n",
      "Recall: 0.0493\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.6040\n",
      "Error 0.3960\n",
      "Precision: 0.4688\n",
      "Recall: 0.0331\n",
      "\n",
      "Random Forest with 100\n",
      "For training set\n",
      "Accuracy 0.6107\n",
      "Error 0.3893\n",
      "Precision: 0.5606\n",
      "Recall: 0.0545\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.6066\n",
      "Error 0.3934\n",
      "Precision: 0.5152\n",
      "Recall: 0.0375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for numOfTrees in [10, 50 ,100]:\n",
    "    trees = randomForest(discreteTrainingX, trainingY, numOfTrees, 8)\n",
    "    \n",
    "    print(\"Random Forest with %d\" % numOfTrees)\n",
    "    #Metrics\n",
    "    y_pred = predictRandomForest(discreteTrainingX, trees) \n",
    "    confMatrix = metrics.confusion_matrix(trainingY, y_pred)\n",
    "    accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "    precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "    recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    print(\"For training set\")\n",
    "    print(\"Accuracy %.4f\" % accuracy)\n",
    "    print(\"Error %.4f\" % (1-accuracy))\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print()\n",
    "\n",
    "\n",
    "    y_pred = predictRandomForest(discreteTestX, trees) \n",
    "    confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "    accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "    precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "    recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    print(\"For testing set\")\n",
    "    print(\"Accuracy %.4f\" % accuracy)\n",
    "    print(\"Error %.4f\" % (1-accuracy))\n",
    "    print(\"Precision: %.4f\" % precision)\n",
    "    print(\"Recall: %.4f\" % recall)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 57 features\n",
      "For training set\n",
      "Accuracy 0.9994\n",
      "Error 0.0006\n",
      "Precision: 0.9993\n",
      "Recall: 0.9993\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9434\n",
      "Error 0.0566\n",
      "Precision: 0.9330\n",
      "Recall: 0.9227\n",
      "\n",
      "Random Forest with 28 features\n",
      "For training set\n",
      "Accuracy 0.9991\n",
      "Error 0.0009\n",
      "Precision: 0.9985\n",
      "Recall: 0.9993\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9504\n",
      "Error 0.0496\n",
      "Precision: 0.9342\n",
      "Recall: 0.9404\n",
      "\n",
      "Random Forest with 7 features\n",
      "For training set\n",
      "Accuracy 0.9991\n",
      "Error 0.0009\n",
      "Precision: 1.0000\n",
      "Recall: 0.9978\n",
      "\n",
      "For testing set\n",
      "Accuracy 0.9539\n",
      "Error 0.0461\n",
      "Precision: 0.9425\n",
      "Recall: 0.9404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sklearn's Random Forest feature varying\n",
    "featuresLen = len(discreteTrainingX[0])\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_features=None)\n",
    "clf = clf.fit(discreteTrainingX, trainingY)\n",
    "\n",
    "print(\"Random Forest with %d features\" % featuresLen)\n",
    "#Metrics\n",
    "y_pred = clf.predict(discreteTrainingX)\n",
    "confMatrix = metrics.confusion_matrix(trainingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For training set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()\n",
    "\n",
    "\n",
    "y_pred = clf.predict(discreteTestX)\n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For testing set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_features=(featuresLen//2))\n",
    "clf = clf.fit(discreteTrainingX, trainingY)\n",
    "\n",
    "print(\"Random Forest with %d features\" % (featuresLen//2))\n",
    "#Metrics\n",
    "y_pred = clf.predict(discreteTrainingX)\n",
    "confMatrix = metrics.confusion_matrix(trainingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For training set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()\n",
    "\n",
    "\n",
    "y_pred = clf.predict(discreteTestX)\n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For testing set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_features=int(featuresLen**(1/2)))\n",
    "clf = clf.fit(discreteTrainingX, trainingY)\n",
    "\n",
    "print(\"Random Forest with %d features\" % int(featuresLen**(1/2)))\n",
    "#Metrics\n",
    "y_pred = clf.predict(discreteTrainingX)\n",
    "confMatrix = metrics.confusion_matrix(trainingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For training set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()\n",
    "\n",
    "\n",
    "y_pred = clf.predict(discreteTestX)\n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "precision = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[0][1])\n",
    "recall = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "print(\"For testing set\")\n",
    "print(\"Accuracy %.4f\" % accuracy)\n",
    "print(\"Error %.4f\" % (1-accuracy))\n",
    "print(\"Precision: %.4f\" % precision)\n",
    "print(\"Recall: %.4f\" % recall)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
